# Pre-training a Tiny GPT from Scratch in a CPU-friendly Setting

## Model Hyperparameters
Layers: 6
Hidden Size: 128
Attention Heads: 2
Sequence Length: 128
Vocabulary Size: 30,000 
